{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CQ2GFDehO3mw",
    "outputId": "480ac255-701b-4f80-b48f-91c436bcffc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Cyrill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Cyrill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cyrill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')#\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')#\n",
    "nltk.download('stopwords') #\n",
    "import sys  \n",
    "#!{sys.executable} -m pip install pyspellchecker \n",
    "#from spellchecker import SpellChecker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the training data. We encode it in utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G6yKi5NROcX3"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',encoding =\"utf-8\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.drop([\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all the characters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"]=train[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#rockyfire update =&gt; california hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i'm on top of the hill and i can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>haha south tampa is getting flooded hah- wait ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #florida #tampabay #tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood in bago myanmar #we arrived bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  our deeds are the reason of this #earthquake m...   \n",
       "1    4     NaN      NaN             forest fire near la ronge sask. canada   \n",
       "2    5     NaN      NaN  all residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  just got sent this photo from ruby #alaska as ...   \n",
       "5    8     NaN      NaN  #rockyfire update => california hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  i'm on top of the hill and i can see a fire in...   \n",
       "8   14     NaN      NaN  there's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  i'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  haha south tampa is getting flooded hah- wait ...   \n",
       "12  18     NaN      NaN  #raining #flooding #florida #tampabay #tampa 1...   \n",
       "13  19     NaN      NaN            #flood in bago myanmar #we arrived bago   \n",
       "14  20     NaN      NaN  damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     what's up man?   \n",
       "16  24     NaN      NaN                                      i love fruits   \n",
       "17  25     NaN      NaN                                   summer is lovely   \n",
       "18  26     NaN      NaN                                  my car is so fast   \n",
       "19  28     NaN      NaN                       what a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the data contains: hashtags (# + text), usernames (@ + username), url's (http or https), contractions (for example: it's), ponctuation, citations that use apostrophes as citations marks ('shelter in place'), numbers and non alpha-numrical characters. In order to have a clean dataset we need to address these by:\n",
    "\n",
    "<ul>\n",
    "  <li>remove usernames, url's ponctuation, numbers, non aplha-numerical characters and apostrophes used as citation marks</li>\n",
    "  <li>replace contractions by their uncontracted version</li>\n",
    "  <li>remove the \"#\" sign from the hashtags, but keep the text (#wildfire => wildfire). </li>\n",
    "</ul>\n",
    "\n",
    "For the replacement of the contractions with their expansions we used a python dictionnary  provided by alko and arturomp @ stack overflow that found here: http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n       \"$\" : \" dollar \",\\n    \"€\" : \" euro \",\\n    \"4ao\" : \"for adults only\",\\n    \"a.m\" : \"before midday\",\\n    \"a3\" : \"anytime anywhere anyplace\",\\n    \"aamof\" : \"as a matter of fact\",\\n    \"acct\" : \"account\",\\n    \"adih\" : \"another day in hell\",\\n    \"afaic\" : \"as far as i am concerned\",\\n    \"afaict\" : \"as far as i can tell\",\\n    \"afaik\" : \"as far as i know\",\\n    \"afair\" : \"as far as i remember\",\\n    \"afk\" : \"away from keyboard\",\\n    \"app\" : \"application\",\\n    \"approx\" : \"approximately\",\\n    \"apps\" : \"applications\",\\n    \"asap\" : \"as soon as possible\",\\n    \"asl\" : \"age, sex, location\",\\n    \"atk\" : \"at the keyboard\",\\n    \"ave.\" : \"avenue\",\\n    \"aymm\" : \"are you my mother\",\\n    \"ayor\" : \"at your own risk\", \\n    \"b&b\" : \"bed and breakfast\",\\n    \"b+b\" : \"bed and breakfast\",\\n    \"b.c\" : \"before christ\",\\n    \"b2b\" : \"business to business\",\\n    \"b2c\" : \"business to customer\",\\n    \"b4\" : \"before\",\\n    \"b4n\" : \"bye for now\",\\n    \"b@u\" : \"back at you\",\\n    \"bae\" : \"before anyone else\",\\n    \"bak\" : \"back at keyboard\",\\n    \"bbbg\" : \"bye bye be good\",\\n    \"bbc\" : \"british broadcasting corporation\",\\n    \"bbias\" : \"be back in a second\",\\n    \"bbl\" : \"be back later\",\\n    \"bbs\" : \"be back soon\",\\n    \"be4\" : \"before\",\\n    \"bfn\" : \"bye for now\",\\n    \"blvd\" : \"boulevard\",\\n    \"bout\" : \"about\",\\n    \"brb\" : \"be right back\",\\n    \"bros\" : \"brothers\",\\n    \"brt\" : \"be right there\",\\n    \"bsaaw\" : \"big smile and a wink\",\\n    \"btw\" : \"by the way\",\\n    \"bwl\" : \"bursting with laughter\",\\n    \"c/o\" : \"care of\",\\n    \"cet\" : \"central european time\",\\n    \"cf\" : \"compare\",\\n    \"cia\" : \"central intelligence agency\",\\n    \"csl\" : \"can not stop laughing\",\\n    \"cu\" : \"see you\",\\n    \"cul8r\" : \"see you later\",\\n    \"cv\" : \"curriculum vitae\",\\n    \"cwot\" : \"complete waste of time\",\\n    \"cya\" : \"see you\",\\n    \"cyt\" : \"see you tomorrow\",\\n    \"dae\" : \"does anyone else\",\\n    \"dbmib\" : \"do not bother me i am busy\",\\n    \"diy\" : \"do it yourself\",\\n    \"dm\" : \"direct message\",\\n    \"dwh\" : \"during work hours\",\\n    \"e123\" : \"easy as one two three\",\\n    \"eet\" : \"eastern european time\",\\n    \"eg\" : \"example\",\\n    \"embm\" : \"early morning business meeting\",\\n    \"encl\" : \"enclosed\",\\n    \"encl.\" : \"enclosed\",\\n    \"etc\" : \"and so on\",\\n    \"faq\" : \"frequently asked questions\",\\n    \"fawc\" : \"for anyone who cares\",\\n    \"fb\" : \"facebook\",\\n    \"fc\" : \"fingers crossed\",\\n    \"fig\" : \"figure\",\\n    \"fimh\" : \"forever in my heart\", \\n    \"ft.\" : \"feet\",\\n    \"ft\" : \"featuring\",\\n    \"ftl\" : \"for the loss\",\\n    \"ftw\" : \"for the win\",\\n    \"fwiw\" : \"for what it is worth\",\\n    \"fyi\" : \"for your information\",\\n    \"g9\" : \"genius\",\\n    \"gahoy\" : \"get a hold of yourself\",\\n    \"gal\" : \"get a life\",\\n    \"gcse\" : \"general certificate of secondary education\",\\n    \"gfn\" : \"gone for now\",\\n    \"gg\" : \"good game\",\\n    \"gl\" : \"good luck\",\\n    \"glhf\" : \"good luck have fun\",\\n    \"gmt\" : \"greenwich mean time\",\\n    \"gmta\" : \"great minds think alike\",\\n    \"gn\" : \"good night\",\\n    \"g.o.a.t\" : \"greatest of all time\",\\n    \"goat\" : \"greatest of all time\",\\n    \"goi\" : \"get over it\",\\n    \"gps\" : \"global positioning system\",\\n    \"gr8\" : \"great\",\\n    \"gratz\" : \"congratulations\",\\n    \"gyal\" : \"girl\",\\n    \"h&c\" : \"hot and cold\",\\n    \"hp\" : \"horsepower\",\\n    \"hr\" : \"hour\",\\n    \"hrh\" : \"his royal highness\",\\n    \"ht\" : \"height\",\\n    \"ibrb\" : \"i will be right back\",\\n    \"ic\" : \"i see\",\\n    \"icq\" : \"i seek you\",\\n    \"icymi\" : \"in case you missed it\",\\n    \"idc\" : \"i do not care\",\\n    \"idgadf\" : \"i do not give a damn fuck\",\\n    \"idgaf\" : \"i do not give a fuck\",\\n    \"idk\" : \"i do not know\",\\n    \"ie\" : \"that is\",\\n    \"i.e\" : \"that is\",\\n    \"ifyp\" : \"i feel your pain\",\\n    \"IG\" : \"instagram\",\\n    \"iirc\" : \"if i remember correctly\",\\n    \"ilu\" : \"i love you\",\\n    \"ily\" : \"i love you\",\\n    \"imho\" : \"in my humble opinion\",\\n    \"imo\" : \"in my opinion\",\\n    \"imu\" : \"i miss you\",\\n    \"iow\" : \"in other words\",\\n    \"irl\" : \"in real life\",\\n    \"j4f\" : \"just for fun\",\\n    \"jic\" : \"just in case\",\\n    \"jk\" : \"just kidding\",\\n    \"jsyk\" : \"just so you know\",\\n    \"l8r\" : \"later\",\\n    \"lb\" : \"pound\",\\n    \"lbs\" : \"pounds\",\\n    \"ldr\" : \"long distance relationship\",\\n    \"lmao\" : \"laugh my ass off\",\\n    \"lmfao\" : \"laugh my fucking ass off\",\\n    \"lol\" : \"laughing out loud\",\\n    \"ltd\" : \"limited\",\\n    \"ltns\" : \"long time no see\",\\n    \"m8\" : \"mate\",\\n    \"mf\" : \"motherfucker\",\\n    \"mfs\" : \"motherfuckers\",\\n    \"mfw\" : \"my face when\",\\n    \"mofo\" : \"motherfucker\",\\n    \"mph\" : \"miles per hour\",\\n    \"mr\" : \"mister\",\\n    \"mrw\" : \"my reaction when\",\\n    \"ms\" : \"miss\",\\n    \"mte\" : \"my thoughts exactly\",\\n    \"nagi\" : \"not a good idea\",\\n    \"nbc\" : \"national broadcasting company\",\\n    \"nbd\" : \"not big deal\",\\n    \"nfs\" : \"not for sale\",\\n    \"ngl\" : \"not going to lie\",\\n    \"nhs\" : \"national health service\",\\n    \"nrn\" : \"no reply necessary\",\\n    \"nsfl\" : \"not safe for life\",\\n    \"nsfw\" : \"not safe for work\",\\n    \"nth\" : \"nice to have\",\\n    \"nvr\" : \"never\",\\n    \"nyc\" : \"new york city\",\\n    \"oc\" : \"original content\",\\n    \"og\" : \"original\",\\n    \"ohp\" : \"overhead projector\",\\n    \"oic\" : \"oh i see\",\\n    \"omdb\" : \"over my dead body\",\\n    \"omg\" : \"oh my god\",\\n    \"omw\" : \"on my way\",\\n    \"p.a\" : \"per annum\",\\n    \"p.m\" : \"after midday\",\\n    \"pm\" : \"prime minister\",\\n    \"poc\" : \"people of color\",\\n    \"pov\" : \"point of view\",\\n    \"pp\" : \"pages\",\\n    \"ppl\" : \"people\",\\n    \"prw\" : \"parents are watching\",\\n    \"ps\" : \"postscript\",\\n    \"pt\" : \"point\",\\n    \"ptb\" : \"please text back\",\\n    \"pto\" : \"please turn over\",\\n    \"qpsa\" : \"what happens\", #\"que pasa\",\\n    \"ratchet\" : \"rude\",\\n    \"rbtl\" : \"read between the lines\",\\n    \"rlrt\" : \"real life retweet\", \\n    \"rofl\" : \"rolling on the floor laughing\",\\n    \"roflol\" : \"rolling on the floor laughing out loud\",\\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\\n    \"rt\" : \"retweet\",\\n    \"ruok\" : \"are you ok\",\\n    \"sfw\" : \"safe for work\",\\n    \"sk8\" : \"skate\",\\n    \"smh\" : \"shake my head\",\\n    \"sq\" : \"square\",\\n    \"srsly\" : \"seriously\", \\n    \"ssdd\" : \"same stuff different day\",\\n    \"tbh\" : \"to be honest\",\\n    \"tbs\" : \"tablespooful\",\\n    \"tbsp\" : \"tablespooful\",\\n    \"tfw\" : \"that feeling when\",\\n    \"thks\" : \"thank you\",\\n    \"tho\" : \"though\",\\n    \"thx\" : \"thank you\",\\n    \"tia\" : \"thanks in advance\",\\n    \"til\" : \"today i learned\",\\n    \"tl;dr\" : \"too long i did not read\",\\n    \"tldr\" : \"too long i did not read\",\\n    \"tmb\" : \"tweet me back\",\\n    \"tntl\" : \"trying not to laugh\",\\n    \"ttyl\" : \"talk to you later\",\\n    \"u\" : \"you\",\\n    \"u2\" : \"you too\",\\n    \"u4e\" : \"yours for ever\",\\n    \"utc\" : \"coordinated universal time\",\\n    \"w/\" : \"with\",\\n    \"w/o\" : \"without\",\\n    \"w8\" : \"wait\",\\n    \"wassup\" : \"what is up\",\\n    \"wb\" : \"welcome back\",\\n    \"wtf\" : \"what the fuck\",\\n    \"wtg\" : \"way to go\",\\n    \"wtpa\" : \"where the party at\",\\n    \"wuf\" : \"where are you from\",\\n    \"wuzup\" : \"what is up\",\\n    \"wywh\" : \"wish you were here\",\\n    \"yd\" : \"yard\",\\n    \"ygtr\" : \"you got that right\",\\n    \"ynk\" : \"you never know\",\\n    \"zzz\" : \"sleeping bored and tired\"\\n\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Contractions = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"gonna\": \"going to\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "       \"$\" : \" dollar \",\n",
    "    \"€\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontraction(total_text):\n",
    "        \n",
    "        for i in Contractions:\n",
    "            total_text = total_text.replace( i, Contractions.get(i))\n",
    "        \n",
    "        return total_text\n",
    "\n",
    "#apply decontraction function to column \"text\"\n",
    "trainS['text'] = trainS['text'].map(decontraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I6sODhizXmO7"
   },
   "outputs": [],
   "source": [
    "def data_text_preprocess(total_text):\n",
    "        \n",
    "        #remove url's of type http or https\n",
    "        total_text = re.sub('http\\S+',' ', str(total_text))\n",
    "        total_test = re.sub('https\\S+',' ', str(total_text))\n",
    "        #remove username\n",
    "        total_text = re.sub('\\@\\S+',' ', str(total_text))\n",
    "        #remove \"#\" sign but keep text of hashtag\n",
    "        total_text = re.sub('\\#',' ', str(total_text))\n",
    "        #remove apostrophes used for citations, which are found at the beginnin or the end of words\n",
    "        total_text = re.sub(r\" \\B'\\b|\\b'\\B\", \" \", str(total_text))\n",
    "        #keep only alphabetical values\n",
    "        total_text = re.sub('[^a-z\\n]', ' ', str(total_text))\n",
    "       \n",
    "        \n",
    "       \n",
    "        return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this  earthquake m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place  are b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive  wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby  alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rockyfire update    california hwy     closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood  disaster heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i m on top of the hill and i can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there s an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i m afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>haha south tampa is getting flooded hah  wait ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>raining  flooding  florida  tampabay  tampa  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>flood in bago myanmar  we arrived bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damage to school bus on    in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what s up man</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what a goooooooaaaaaal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  our deeds are the reason of this  earthquake m...   \n",
       "1    4     NaN      NaN             forest fire near la ronge sask  canada   \n",
       "2    5     NaN      NaN  all residents asked to shelter in place  are b...   \n",
       "3    6     NaN      NaN         people receive  wildfires evacuation or...   \n",
       "4    7     NaN      NaN  just got sent this photo from ruby  alaska as ...   \n",
       "5    8     NaN      NaN   rockyfire update    california hwy     closed...   \n",
       "6   10     NaN      NaN   flood  disaster heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  i m on top of the hill and i can see a fire in...   \n",
       "8   14     NaN      NaN  there s an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  i m afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  haha south tampa is getting flooded hah  wait ...   \n",
       "12  18     NaN      NaN   raining  flooding  florida  tampabay  tampa  ...   \n",
       "13  19     NaN      NaN             flood in bago myanmar  we arrived bago   \n",
       "14  20     NaN      NaN  damage to school bus on    in multi car crash ...   \n",
       "15  23     NaN      NaN                                     what s up man    \n",
       "16  24     NaN      NaN                                      i love fruits   \n",
       "17  25     NaN      NaN                                   summer is lovely   \n",
       "18  26     NaN      NaN                                  my car is so fast   \n",
       "19  28     NaN      NaN                       what a goooooooaaaaaal         \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'] = train['text'].map(data_text_preprocess)\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "After basic cleaning we can proceed by lemmatizing the text. In order to do the most effective lemmatization we need to identify the part-of-speech of every word (noun, verb, adjective, adverb). If we don't do this, a lot of verbs in present continuous tense (-ing) are identified as nouns ending with \"ing\"(e.g. uprising) and won't be lemmatized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "   \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization1(total_text):\n",
    "    #in this function we do tokenization and lemmatization simultaneously\n",
    "    total_text = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(total_text)]\n",
    "\n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[our, deed, be, the, reason, of, this, earthqu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[all, resident, ask, to, shelter, in, place, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[just, get, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[rockyfire, update, california, hwy, close, in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flood, disaster, heavy, rain, cause, flash, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, m, on, top, of, the, hill, and, i, can, se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[there, s, an, emergency, evacuation, happen, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, m, afraid, that, the, tornado, be, come, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[three, people, die, from, the, heat, wave, so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[haha, south, tampa, be, get, flood, hah, wait...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[rain, flood, florida, tampabay, tampa, or, da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flood, in, bago, myanmar, we, arrive, bago]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[damage, to, school, bus, on, in, multi, car, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[what, s, up, man]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, love, fruit]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[summer, be, lovely]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[my, car, be, so, fast]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[what, a, goooooooaaaaaal]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  [our, deed, be, the, reason, of, this, earthqu...   \n",
       "1    4     NaN      NaN      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2    5     NaN      NaN  [all, resident, ask, to, shelter, in, place, b...   \n",
       "3    6     NaN      NaN  [people, receive, wildfire, evacuation, order,...   \n",
       "4    7     NaN      NaN  [just, get, sent, this, photo, from, ruby, ala...   \n",
       "5    8     NaN      NaN  [rockyfire, update, california, hwy, close, in...   \n",
       "6   10     NaN      NaN  [flood, disaster, heavy, rain, cause, flash, f...   \n",
       "7   13     NaN      NaN  [i, m, on, top, of, the, hill, and, i, can, se...   \n",
       "8   14     NaN      NaN  [there, s, an, emergency, evacuation, happen, ...   \n",
       "9   15     NaN      NaN  [i, m, afraid, that, the, tornado, be, come, t...   \n",
       "10  16     NaN      NaN  [three, people, die, from, the, heat, wave, so...   \n",
       "11  17     NaN      NaN  [haha, south, tampa, be, get, flood, hah, wait...   \n",
       "12  18     NaN      NaN  [rain, flood, florida, tampabay, tampa, or, da...   \n",
       "13  19     NaN      NaN       [flood, in, bago, myanmar, we, arrive, bago]   \n",
       "14  20     NaN      NaN  [damage, to, school, bus, on, in, multi, car, ...   \n",
       "15  23     NaN      NaN                                 [what, s, up, man]   \n",
       "16  24     NaN      NaN                                   [i, love, fruit]   \n",
       "17  25     NaN      NaN                               [summer, be, lovely]   \n",
       "18  26     NaN      NaN                            [my, car, be, so, fast]   \n",
       "19  28     NaN      NaN                         [what, a, goooooooaaaaaal]   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'] = train['text'].map(lemmatization1)\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are non-relevant for our text analysis and will overload our word vectors, so we get rid of them by using a predefined stopword list provided by the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(total_text):\n",
    "    \n",
    "    filtered_word_list = total_text[:]\n",
    "    for word in total_text:\n",
    "        if word in stop_words: \n",
    "            filtered_word_list.remove(word)\n",
    "    \n",
    "    #filtered_sentence = [w for w in total_text if not w in stop_words] \n",
    "    #filtered_sentence= []\n",
    "    \n",
    "    #for w in total_text: \n",
    "      #  if w not in stop_words: \n",
    "       #     filtered_sentence.append(w) \n",
    "\n",
    "    return filtered_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[get, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[rockyfire, update, california, hwy, close, di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flood, disaster, heavy, rain, cause, flash, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[top, hill, see, fire, wood]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[emergency, evacuation, happen, building, acro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[afraid, tornado, come, area]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[three, people, die, heat, wave, far]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[haha, south, tampa, get, flood, hah, wait, se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[rain, flood, florida, tampabay, tampa, day, l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[flood, bago, myanmar, arrive, bago]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[damage, school, bus, multi, car, crash, break]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[man]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[love, fruit]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[summer, lovely]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[car, fast]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[goooooooaaaaaal]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  [deed, reason, earthquake, may, allah, forgive...   \n",
       "1    4     NaN      NaN      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2    5     NaN      NaN  [resident, ask, shelter, place, notify, office...   \n",
       "3    6     NaN      NaN  [people, receive, wildfire, evacuation, order,...   \n",
       "4    7     NaN      NaN  [get, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "5    8     NaN      NaN  [rockyfire, update, california, hwy, close, di...   \n",
       "6   10     NaN      NaN  [flood, disaster, heavy, rain, cause, flash, f...   \n",
       "7   13     NaN      NaN                       [top, hill, see, fire, wood]   \n",
       "8   14     NaN      NaN  [emergency, evacuation, happen, building, acro...   \n",
       "9   15     NaN      NaN                      [afraid, tornado, come, area]   \n",
       "10  16     NaN      NaN              [three, people, die, heat, wave, far]   \n",
       "11  17     NaN      NaN  [haha, south, tampa, get, flood, hah, wait, se...   \n",
       "12  18     NaN      NaN  [rain, flood, florida, tampabay, tampa, day, l...   \n",
       "13  19     NaN      NaN               [flood, bago, myanmar, arrive, bago]   \n",
       "14  20     NaN      NaN    [damage, school, bus, multi, car, crash, break]   \n",
       "15  23     NaN      NaN                                              [man]   \n",
       "16  24     NaN      NaN                                      [love, fruit]   \n",
       "17  25     NaN      NaN                                   [summer, lovely]   \n",
       "18  26     NaN      NaN                                        [car, fast]   \n",
       "19  28     NaN      NaN                                  [goooooooaaaaaal]   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'] = train['text'].map(remove_stopwords)\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the cleaning we've done until now results in some tokens containing only one character. Since they haven't got any meaning, we remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removesinglelettertoken(total_text):\n",
    "    emptylist=[]\n",
    "    for w in total_text:\n",
    "        if len(w)>1:\n",
    "            emptylist.append(w)\n",
    "    return emptylist\n",
    "\n",
    "train['text'] = train['text'].map(removesinglelettertoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spell = SpellChecker()\n",
    "\n",
    "#def spellchecker(total_text):\n",
    "   # for w in total_text: \n",
    "     #   spell.correction(w) \n",
    "   # return total_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the whole dataset, we remarked that there are duplicates in the column \"text\". While some of them are perfect duplicates (column \"text\" AND column \"target\" are the same), others contain contradictions in the target column. This can lead to a less optimal estimation of the words' coefficients and an overall poorer score. How did we check if a group of duplicated tweets contains contradictions? We calculated the mean of the targets for each duplicate group. Means of 1 or 0 indicate perfect duplicates without contradictions. Meanwhile 0 < mean < 1 indicates that a duplicate group contains at least one contradiction. Imperfect duplicates that affect negatively our analysis are those, that have a close to or perfectly balanced number of 0 and 1 targets (e.g. 11 duplicates, 6 have target = 0 , 5 have target = 1). The more balanced the contradicting target values are, the closer the mean is to 0.5. We dediced to only keep imperfect duplicates with mean < 0.3 or 0.7 < mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_duplicate function only works with type string or int, so we need to reconvert the list of tokens to a single string\n",
    "\n",
    "def reconcatenate(total_text):\n",
    "    TOTAL_text=''\n",
    "    for w in total_text:\n",
    "        TOTAL_text=TOTAL_text + w + ' '\n",
    "    return TOTAL_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN          deed reason earthquake may allah forgive    \n",
       "1   4     NaN      NaN             forest fire near la ronge sask canada    \n",
       "2   5     NaN      NaN  resident ask shelter place notify officer evac...   \n",
       "3   6     NaN      NaN  people receive wildfire evacuation order calif...   \n",
       "4   7     NaN      NaN  get sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text']=train['text'].map(reconcatenate)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyrill\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\Cyrill\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN          deed reason earthquake may allah forgive    \n",
       "1   4     NaN      NaN             forest fire near la ronge sask canada    \n",
       "2   5     NaN      NaN  resident ask shelter place notify officer evac...   \n",
       "3   6     NaN      NaN  people receive wildfire evacuation order calif...   \n",
       "4   7     NaN      NaN  get sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "   target  mean  \n",
       "0       1     1  \n",
       "1       1     1  \n",
       "2       1     1  \n",
       "3       1     1  \n",
       "4       1     1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['mean'] = train.groupby('text')['target'].transform(np.mean)\n",
    "\n",
    "\n",
    "\n",
    "#keep duplicates that have target-mean smaller than 0.3 or greater than 0.7\n",
    "TRAIN=train[(train['mean']>0.7) | (train['mean']<0.3)]\n",
    "TRAIN['mean']=TRAIN['mean'].map(round)\n",
    "#keep only one entry of perfect duplicates\n",
    "TRAIN.drop_duplicates(subset =\"text\", keep = 'first', inplace = True)\n",
    "TRAIN.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize(total_text):\n",
    "    nltk_tokens = nltk.word_tokenize(total_text)\n",
    "    return nltk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyrill\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "TRAIN['text']=TRAIN['text'].map(retokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "TRAIN.to_csv(\"trainclean.csv\", index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do same cleaning procedure (except for the duplicate treatment part) with the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data and put to lowercase\n",
    "test = pd.read_csv('test.csv',encoding =\"utf-8\" )\n",
    "test[\"text\"]=test[\"text\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [decontraction, data_text_preprocess, lemmatization1, remove_stopwords, removesinglelettertoken]:\n",
    "    test['text']=test[\"text\"].map(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "test.to_csv(\"testclean.csv\", index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle project .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
